{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*special thanks to YSDA team for provided materials*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes today:\n",
    "- Introduction to PyTorch\n",
    "- Automatic gradient computation\n",
    "- Logistic regression (it's a neural network, actually ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "\n",
    "__This notebook__ will teach you to use pytorch low-level core. You can install it [here](http://pytorch.org/).\n",
    "\n",
    "__Pytorch feels__ differently than other frameworks (like tensorflow/theano) on almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4,4)\n",
    "\n",
    "print(\"X :\\n%s\\n\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print(\"add 5 :\\n%s\\n\" % (x + 5))\n",
    "print(\"X*X^T  :\\n%s\\n\" % np.dot(x,x.T))\n",
    "print(\"mean over cols :\\n%s\\n\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\\n\" % (np.cumsum(x,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4,4)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32) #or torch.arange(0,16).view(4,4)\n",
    "\n",
    "print (\"X :\\n%s\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print (\"add 5 :\\n%s\" % (x + 5))\n",
    "print (\"X*X^T  :\\n%s\" % torch.matmul(x,x.transpose(1,0)))  #short: x.mm(x.t())\n",
    "print (\"mean over cols :\\n%s\" % torch.mean(x,dim=-1))\n",
    "print (\"cumsum of cols :\\n%s\" % torch.cumsum(x,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy and Pytorch\n",
    "\n",
    "As you can notice, pytorch allows you to hack stuff much the same way you did with numpy. This means that you can _see the numeric value of any tensor at any moment of time_. Debugging such code can be done with by printing tensors or using any debug tool you want (e.g. [gdb](https://wiki.python.org/moin/DebuggingWithGdb)).\n",
    "\n",
    "You could also notice the a few new method names and a different API. So no, there's no compatibility with numpy [yet](https://github.com/pytorch/pytorch/issues/2228) and yes, you'll have to memorize all the names again. Get excited!\n",
    "\n",
    "![img](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2R7-3j6dzcu8uvD0MoNVO9KSmKVRrYJ4hTThAhuvbPo0Q6Lws)\n",
    "\n",
    "For example, \n",
    "* If something takes a list/tuple of axes in numpy, you can expect it to take *args in pytorch\n",
    " * `x.reshape([1,2,8]) -> x.view(1,2,8)`\n",
    "* You should swap _axis_ for _dim_ in operations like mean or cumsum\n",
    " * `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "* most mathematical operations are the same, but types an shaping is different\n",
    " * `x.astype('int64') -> x.type(torch.LongTensor)`\n",
    "\n",
    "To help you acclimatize, there's a [table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) covering most new things. There's also a neat [documentation page](http://pytorch.org/docs/master/).\n",
    "\n",
    "Finally, if you're stuck with a technical problem, we recommend searching [pytorch forumns](https://discuss.pytorch.org/). Or just googling, which usually works just as efficiently. \n",
    "\n",
    "If you feel like you almost give up, remember two things: __GPU__ an __free gradients__. Besides you can always jump back to numpy with x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup: trigonometric knotwork\n",
    "_inspired by [this post](https://www.quora.com/What-are-the-most-interesting-equation-plots)_\n",
    "\n",
    "There are some simple mathematical functions with cool plots. For one, consider this:\n",
    "\n",
    "$$ x(t) = t - 1.5 * cos( 15 t) $$\n",
    "$$ y(t) = t - 1.5 * sin( 16 t) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "t = torch.linspace(-10, 10, steps = 10000)\n",
    "\n",
    "# compute x(t) and y(t) as defined above\n",
    "x = <your_code_here>\n",
    "y = <your_code_here>\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you're done early, try adjusting the formula and seing how  it affects the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic gradients\n",
    "\n",
    "Any self-respecting DL framework must do your backprop for you. Torch handles this with the `autograd` module.\n",
    "\n",
    "The general pipeline looks like this:\n",
    "* When creating a tensor, you mark it as `requires_grad`:\n",
    "    * __```torch.zeros(5, requires_grad=True)```__\n",
    "    * torch.tensor(np.arange(5), dtype=torch.float32, requires_grad=True)\n",
    "* Define some differentiable `loss = arbitrary_function(a)`\n",
    "* Call `loss.backward()`\n",
    "* Gradients are now available as ```a.grads```\n",
    "\n",
    "__Here's an example:__ let's fit a linear regression on Boston house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:,-1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "loss = torch.mean( (y_pred - y)**2 )\n",
    "\n",
    "# propagete gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are now stored in `.grad` of those variables that require them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dL/dw = {}\\n\".format(w.grad))\n",
    "print(\"dL/db = {}\\n\".format(b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute gradient from multiple losses, the gradients will add up at variables, therefore it's useful to __zero the gradients__ between iteratons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    y_pred = w * x  + b\n",
    "    loss = torch.mean( (y_pred - y)**2 )\n",
    "    loss.backward()\n",
    "\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i+1)%5==0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.scatter(x.data.numpy(), y_pred.data.numpy(), color='orange', linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy())\n",
    "        if loss.data.numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quest__: try implementing and writing some nonlinear regression. You can try quadratic features or some trigonometry, or a simple neural network. The only difference is that now you have more variables and a more complicated `y_pred`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember!**\n",
    "![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n",
    "\n",
    "When dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level pytorch\n",
    "\n",
    "So far we've been dealing with low-level torch API. While it's absolutely vital for any custom losses or layers, building large neura nets in it is a bit clumsy.\n",
    "\n",
    "Luckily, there's also a high-level torch interface with a pre-defined layers, activations and training algorithms. \n",
    "\n",
    "We'll cover them as we go through a simple image recognition problem: classifying letters into __\"A\"__ vs __\"B\"__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notmnist import load_notmnist\n",
    "X_train, y_train, X_test, y_test = load_notmnist(letters='AB')\n",
    "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])\n",
    "\n",
    "print(\"Train size = %i, test_size = %i\"%(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(X_train[i].reshape([28,28]))\n",
    "    plt.title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with layers. The main abstraction here is __`torch.nn.Module`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a vast library of popular layers and architectures already built for ya'.\n",
    "\n",
    "This is a binary classification problem, so we'll train a __Logistic Regression with sigmoid__.\n",
    "$$P(y_i | X_i) = \\sigma(W \\cdot X_i + b) ={ 1 \\over {1+e^{- [W \\cdot X_i + b]}} }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network that stacks layers on top of each other\n",
    "model = nn.Sequential()\n",
    "\n",
    "# add first \"dense\" layer with 784 input units and 1 output unit. \n",
    "model.add_module('l1', nn.Linear(784, 1))\n",
    "\n",
    "# add softmax activation for probabilities. Normalize over axis 1\n",
    "# note: layer names must be unique\n",
    "model.add_module('l2', nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data with 3 samples and 784 features\n",
    "x = torch.tensor(X_train[:3], dtype=torch.float32)\n",
    "y = torch.tensor(y_train[:3], dtype=torch.float32)\n",
    "\n",
    "# compute outputs given inputs, both are variables\n",
    "y_predicted = model(x)[:, 0]\n",
    "\n",
    "y_predicted # display what we've got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a loss function for our model.\n",
    "\n",
    "The natural choice is to use binary crossentropy (aka logloss, negative llh):\n",
    "$$ L = {1 \\over N} \\underset{X_i,y_i} \\sum - [  y_i \\cdot log P(y_i | X_i) + (1-y_i) \\cdot log (1-P(y_i | X_i)) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossentropy = ### YOUR CODE\n",
    "\n",
    "loss = ### YOUR CODE\n",
    "\n",
    "assert tuple(crossentropy.size()) == (3,), \"Crossentropy must be a vector with element per sample\"\n",
    "assert tuple(loss.size()) == (1,), \"Loss must be scalar. Did you forget the mean/sum?\"\n",
    "assert loss.data.numpy()[0] > 0, \"Crossentropy must non-negative, zero only for perfect prediction\"\n",
    "assert loss.data.numpy()[0] <= np.log(3), \"Loss is too large even for untrained model. Please double-check it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ you can also find many such functions in `torch.nn.functional`, just type __`F.<tab>`__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Torch optimizers__\n",
    "\n",
    "When we trained Linear Regression above, we had to manually .zero_() gradients on both our variables. Imagine that code for a 50-layer network.\n",
    "\n",
    "Again, to keep it from getting dirty, there's `torch.optim` module with pre-implemented algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "# here's how it's used:\n",
    "loss.backward()      # add new gradients\n",
    "opt.step()           # change weights\n",
    "opt.zero_grad()      # clear gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispose of old variables to avoid bugs later\n",
    "del x, y, y_predicted, loss, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential()\n",
    "model.add_module('first', nn.Linear(784, 1))\n",
    "model.add_module('second', nn.Sigmoid())\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # sample 256 random images\n",
    "    ix = np.random.randint(0, len(X_train), 256)\n",
    "    x_batch = torch.tensor(X_train[ix], dtype=torch.float32)\n",
    "    y_batch = torch.tensor(y_train[ix], dtype=torch.float32)\n",
    "    \n",
    "    # predict probabilities\n",
    "    y_predicted = ### YOUR CODE\n",
    "    \n",
    "    assert y_predicted.dim() == 1, \"did you forget to select first column with [:, 0]\"\n",
    "    \n",
    "    # compute loss, just like before\n",
    "    loss = ### YOUR CODE\n",
    "    \n",
    "    # compute gradients\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Adam step\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # clear gradients\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Debugging tips:__\n",
    "* make sure your model predicts probabilities correctly. Just print them and see what's inside.\n",
    "* don't forget _minus_ sign in the loss function! It's a mistake 99% ppl do at some point.\n",
    "* make sure you zero-out gradients after each step. Srsly:)\n",
    "* In general, pytorch's error messages are quite helpful, read 'em before you google 'em.\n",
    "* if you see nan/inf, print what happens at each iteration to find our where exactly it occurs.\n",
    "  * If loss goes down and then turns nan midway through, try smaller learning rate. (Our current loss formula is unstable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's see how our model performs on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your model to predict classes (0 or 1) for all test samples\n",
    "predicted_y_test = ### YOUR CODE\n",
    "predicted_y_test = np.array(predicted_y_test > 0.5)\n",
    "         \n",
    "assert isinstance(predicted_y_test, np.ndarray), \"please return np array, not %s\" % type(predicted_y_test)\n",
    "assert predicted_y_test.shape == y_test.shape, \"please predict one class for each test sample\"\n",
    "assert np.in1d(predicted_y_test, y_test).all(), \"please predict class indexes\"\n",
    "\n",
    "accuracy = np.mean(predicted_y_test == y_test)\n",
    "\n",
    "print(\"Test accuracy: %.5f\" % accuracy)\n",
    "assert accuracy > 0.95, \"try training longer\"\n",
    "\n",
    "print('Great job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To sum up:\n",
    "- Welcome to the world of the Deep Learning! \n",
    "- More complex neural nets, layers and tricks are coming next.\n",
    "\n",
    "Leave feedback, please!\n",
    "http://bit.ly/ml4sber_july18_seminar6_feedback\n",
    "\n",
    "And now take your time and grab something sweet at lunch :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "### More about pytorch:\n",
    "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 torch",
   "language": "python",
   "name": "py3_research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
