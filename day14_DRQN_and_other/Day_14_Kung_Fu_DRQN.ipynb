{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 14: DRQN\n",
    "This notebook is based on YSDA Practical_RL [week08](https://github.com/yandexdataschool/Practical_RL/tree/master/week08_pomdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ../xvfb: No such file or directory\n",
      "env: DISPLAY=: 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# if you're running in colab\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![img0](kung_fu_img.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv1 = < YOUR CODE >\n",
    "        self.conv2 = < YOUR CODE >\n",
    "        self.conv3 = < YOUR CODE >\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = < YOUR CODE >\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_agent_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "#           <YOUR CODE >\n",
    "        \n",
    "        new_agent_state = <YOUR CODE >\n",
    "        logits = <YOUR CODE >\n",
    "        agent_state_value = <YOUR CODE >\n",
    "\n",
    "        return new_agent_state, (logits, agent_state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_agent_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is a numpy array \"\"\"\n",
    "        obs_t = torch.tensor(np.asarray(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_agent_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer tensor and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.to(dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = torch.tensor(np.asarray(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)  # shape: [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    is_not_done = torch.tensor(np.array(is_not_done), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = <YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += <YOUR CODE >\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = <YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += <YOUR CODE >\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = <compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    < your code >\n",
    "\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in trange(15000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or PyTorch developers. Or aliens. Or lizardfolk. Contact us before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus area:\n",
    "1. Try to add attention layer to the Q-network.\n",
    "2. Try applying the same Approach to Atari Breakout or other environments.\n",
    "3. Try implementing ACER ([arxiv paper](https://arxiv.org/abs/1611.01224) and [github repo](https://github.com/Kaixhin/ACER)).\n",
    "4. Try implementing TRPO (Trust Region Policy Optimization). [Link](https://spinningup.openai.com/en/latest/algorithms/trpo.html) to comprehensive overview by OpenAI.\n",
    "5. Try implementing PPO (Proximal Policy Optimization) - one of the most popular approaches these days. Here is the [link](https://spinningup.openai.com/en/latest/algorithms/ppo.html) to OpenAI overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
