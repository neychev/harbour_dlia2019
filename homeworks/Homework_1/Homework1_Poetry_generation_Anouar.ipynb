{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niVXDzbAh3JA"
   },
   "source": [
    "## Homework №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEdAbVuWh3JC"
   },
   "source": [
    "### Part 1. Almost Shakespeare\n",
    "\n",
    "Let's try to generate some Shakespeare poetry using RNNs. The sonnets file is available in the notebook directory.\n",
    "\n",
    "Text generation can be designed in several steps:\n",
    "    \n",
    "1. Data loading.\n",
    "2. Dictionary generation.\n",
    "3. Data preprocessing.\n",
    "4. Model (neural network) training.\n",
    "5. Text generation (model evaluation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0JQ6Qoph3JE"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tez8mTN3h3JF"
   },
   "source": [
    "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`).\n",
    "\n",
    "Simple preprocessing is already done for you in the next cell: all technical info is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "1XIRXD30i2Uk",
    "outputId": "41d3218b-7a3f-4b82-c523-98cce02fde6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-16 14:21:38--  http://www.gutenberg.org/cache/epub/1041/pg1041.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 122780 (120K) [text/plain]\n",
      "Saving to: ‘pg1041.txt’\n",
      "\n",
      "pg1041.txt          100%[===================>] 119.90K   559KB/s    in 0.2s    \n",
      "\n",
      "2019-07-16 14:21:38 (559 KB/s) - ‘pg1041.txt’ saved [122780/122780]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://www.gutenberg.org/cache/epub/1041/pg1041.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24dbGjUhh3JH"
   },
   "outputs": [],
   "source": [
    "with open('sonnets.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "TEXT_START = 45\n",
    "TEXT_END = -368\n",
    "text = text[TEXT_START : TEXT_END]\n",
    "assert len(text) == 2616\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIUjqhfgh3JO"
   },
   "source": [
    "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
    "\n",
    "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RC1bofhQh3JP",
    "outputId": "cf6767fd-1c2f-442d-e696-e2ccd0aebf0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97989"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join all the strings into one and lowercase it\n",
    "# Put result into variable text.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
    "text = [preprocess(x) for x in text ]\n",
    "text = ' '.join(text)\n",
    "len(text)\n",
    "# Your great code here\n",
    "\n",
    "# assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
    "# assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
    "# print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGT41HRxh3JU"
   },
   "source": [
    "Put all the characters, that you've seen in the text, into variable `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBXeiAVkh3JV"
   },
   "outputs": [],
   "source": [
    "tokens = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAnSUVrMh3JZ"
   },
   "source": [
    "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRycX54lh3Ja"
   },
   "outputs": [],
   "source": [
    "# dict <index>:<char>\n",
    "# Your great code here\n",
    "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "\n",
    "# dict <char>:<index>\n",
    "# Your great code here\n",
    "\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRJExBr9h3Jf"
   },
   "source": [
    "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9w_GrL-Uh3Jh"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dm3OGqFih3Jj"
   },
   "source": [
    "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
    "\n",
    "Let's use vanilla RNN, similar to the one created during the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-KmJWiI4RdY"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLVZXZ6wh3Jl"
   },
   "outputs": [],
   "source": [
    "class CharRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the scheme above as torch module\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tokens=len(text), embedding_size=16, rnn_num_units=64):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "        \n",
    "        :param x: batch of character ids, variable containing vector of int64\n",
    "        :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n",
    "        \"\"\"\n",
    "        # get vector embedding of x\n",
    "        x_emb = self.embedding(x)\n",
    "        \n",
    "        # compute next hidden state using self.rnn_update\n",
    "        # hint: use torch.cat(..., dim=...) for concatenation\n",
    "        x_and_h = torch.cat((x_emb, h_prev), dim=1)\n",
    "        h_next = self.rnn_update(x_and_h)\n",
    "        \n",
    "        h_next = F.tanh(h_next)\n",
    "        \n",
    "        assert h_next.size() == h_prev.size()\n",
    "        \n",
    "        #compute logits for next character probs\n",
    "        logits = self.rnn_to_logits(h_next)\n",
    "    \n",
    "        return h_next, F.log_softmax(logits, -1)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return Variable(torch.zeros(batch_size, self.num_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQDVHO_14df5"
   },
   "outputs": [],
   "source": [
    "char_rnn = CharRNNCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7-SmfqW4jbs"
   },
   "outputs": [],
   "source": [
    "def rnn_loop(char_rnn, batch_ix):\n",
    "    \"\"\"\n",
    "    Computes log P(next_character) for all time-steps in names_ix\n",
    "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
    "    \"\"\"\n",
    "    batch_size, max_length = batch_ix.size()\n",
    "    hid_state = char_rnn.initial_state(batch_size)\n",
    "    logprobs = []\n",
    "\n",
    "    for x_t in batch_ix.transpose(0,1):\n",
    "        hid_state, logp_next = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
    "        logprobs.append(logp_next)\n",
    "        \n",
    "    return torch.stack(logprobs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxQABaXD5vWo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zDLD_b8Jh3Jr"
   },
   "source": [
    "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWfu8xV56UKd"
   },
   "outputs": [],
   "source": [
    "def to_matrix(names, max_len=None, pad=token_to_idx[' '], dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        line_ix = [token_to_idx[c] for c in names[i]]\n",
    "        names_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        names_ix = np.transpose(names_ix)\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "sJSCtsQNh3Jt",
    "outputId": "bc45d3c7-551f-44d0-f7a6-8085bc8fd870"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e8e696cfef2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# train with backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_research/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_research/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "opt = torch.optim.Adam(char_rnn.parameters())\n",
    "history = []\n",
    "\n",
    "MAX_LENGTH = 16\n",
    "\n",
    "for i in range(500):\n",
    "    batch_ix = to_matrix(sample(text, 32), max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "    \n",
    "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    #<YOUR CODE>\n",
    "    predictions_logp = logp_seq[:, :-1]\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))###YOUR CODE\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    #<YOUR CODE>\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%5==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "uPPUCiqgh3Jz",
    "outputId": "53e5f39d-9920-4fba-f84b-46eae7c5d97e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nexes/miniconda3/envs/py3_research/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "# An example of generated text. There is no function `generate_text` in the code above.\n",
    "num_tokens = len(text)\n",
    "\n",
    "def generate_text(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_idx[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
    "    hid_state = char_rnn.initial_state(batch_size=1)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(num_tokens,p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])\n",
    "  \n",
    "  \n",
    "print(generate_text(char_rnn, max_length=500, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PBgT39uh3J9"
   },
   "source": [
    "### More poetic model\n",
    "\n",
    "Let's use LSTM instead of vanilla RNN and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYUGrS0Ah3J-"
   },
   "source": [
    "Plot the loss function of the number of epochs. Does the final loss become better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRiyIokXKErk"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfBUhRnVKGtw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WW1RvOx48k82"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CharLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements something like CharRNNCell, but with LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tokens=len(text), embedding_size=16, rnn_num_units=64):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.emb = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.lstm = nn.LSTMCell(embedding_size, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        (prev_h, prev_c) = prev_state\n",
    "        (next_h, next_c) = self.lstm(self.emb(x), (prev_h, prev_c))\n",
    "        logits = self.rnn_to_logits(next_h)\n",
    "        \n",
    "        return (next_h, next_c), F.log_softmax(logits, -1)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" LSTM has two state variables, cell and hid \"\"\"\n",
    "        return Variable(torch.zeros(batch_size, self.num_units)), Variable(torch.zeros(batch_size, self.num_units))\n",
    "    \n",
    "char_lstm = CharLSTMCell().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "id": "OLDI0Lmbh3J_",
    "outputId": "bbd0f926-cf12-4a5e-ef93-1c7edb34875f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-96c714ebb649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlogp_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# compute loss. This time we use nll_loss with some duct tape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-50e08883510d>\u001b[0m in \u001b[0;36mrnn_loop\u001b[0;34m(char_rnn, batch_ix)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- here we call your one-step code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlogprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0e01cbff229c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mprev_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mnext_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_to_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m         )\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 16\n",
    "\n",
    "for i in range(500):\n",
    "    print(i)\n",
    "    \n",
    "    batch_ix = to_matrix(sample(text, 32), max_len=MAX_LENGTH)\n",
    "    batch_ix = Variable(torch.LongTensor(batch_ix)).to(device)\n",
    "\n",
    "    logp_seq = rnn_loop(char_lstm, batch_ix)\n",
    "\n",
    "    # compute loss. This time we use nll_loss with some duct tape\n",
    "    loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, len(text)), batch_ix[:, :-1].contiguous().view(-1))\n",
    "\n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    #<YOUR CODE>\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27MaQBcHh3KD"
   },
   "source": [
    "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
    "\n",
    "Evaluate the results visually, try to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSUkXGjOh3KE"
   },
   "outputs": [],
   "source": [
    "# Text generation with different temperature values here\n",
    "for i in  [0.1, 0.2, 0.5, 1.0, 2.0]: \n",
    "    print(generate_text(char_lstm, length=500, temperature=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qc73ahF-h3KJ"
   },
   "source": [
    "### Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSk8_oQth3KK"
   },
   "source": [
    "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VTJofW8h3KL"
   },
   "outputs": [],
   "source": [
    "# Saving and loading code here\n",
    "torch.save(model, r\"C:\\Program Files\\iTunes\\hw.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMFUcYL7Jh4g"
   },
   "outputs": [],
   "source": [
    "model = torch.load(r\"C:\\Program Files\\iTunes\\hw.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGDy9R6-h3KP"
   },
   "source": [
    "### References\n",
    "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
    "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
    "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
    "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Homework1_Poetry_generation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
